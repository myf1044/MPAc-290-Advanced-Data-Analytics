# -*- coding: utf-8 -*-
"""MPAc290 8th session.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14tyGU6fX0hWRB-Am1XZa4C4foaK5dWai

using ANNs for classification
"""

import matplotlib.pyplot as plt  
import pandas as pd  
#importing the necessary packages  
from sklearn.model_selection import train_test_split  
from sklearn.naive_bayes import GaussianNB 
from sklearn.naive_bayes import MultinomialNB 

from inspect import signature

import sklearn
from sklearn import linear_model, dummy, metrics
from sklearn.dummy import DummyClassifier
from sklearn.metrics import *

import seaborn as sns

#downloading the iris dataset, splitting it into train set and validation set 
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
col=['sepal_length','sepal_width','petal_length','petal_width','type']

df = pd.read_csv(url, names = col)

# Print first 5 rows of the dataframe
df.head()

df['type'].unique()

df2 = df
df2['type']  = df2['type'].replace({'Iris-setosa': 0})
df2['type']  = df2['type'].replace({'Iris-versicolor': 1})
df2['type']  = df2['type'].replace({'Iris-virginica': 2})
df2.head()

X = df2[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']] #predictors

y = df2[['type']] #target

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

# Bivariate Pairwise relationships between columns with seaborn library
sns.pairplot(df2, hue="type", size=3, diag_kind="kde")

from sklearn.neural_network import MLPClassifier
mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10000)
mlp.fit(X_train, y_train.values.ravel())

y_predict = mlp.predict(X_test)  
print("Accuracy MLP: {:.5f}".format(mlp.score(X_test, y_test)))

plt.plot(mlp.loss_curve_)
plt.show()

mlp.classes_

y_predict = pd.DataFrame(y_predict,index = y_test.index, columns = ['predict'])

y_predict

dftest = pd.concat([y_test, y_predict['predict']])
df3 = dftest.head(25)
df3.head()

#Build a "dummy" classifier that predicts every observation to be the most frequent class, in this case 'no-exit')
dummy = DummyClassifier(strategy=  "prior")#, Constant = 1)
dumb = dummy.fit(X_train,y_train)
dumb_pred = dumb.predict(X_test)
dumb_pred_prob = dumb.predict_proba(X_test)

dumb_pred = pd.DataFrame(dumb_pred,index = y_test.index, columns = ['predict'])

print("Test accuracy (ANN) =", mlp.score(X_test,y_test), end="\n\n")

print("Test accuracy (Dummy Classifier) =", dumb.score(dumb_pred,y_test))

#generate confusion matrix for Dummy classifier
cm_dummy = confusion_matrix(y_test, dumb_pred)
#put it into a dataframe for seaborn plot function
cm_dummy_df = pd.DataFrame(cm_dummy)

#Use a seaborn heatmap to plot confusion matrices
#The dataframe is transposed to make Actual values on x-axis and predicted on y-axis
#annot = True includes the numbers in each box
#vmin and vmax just adjusts the color value
fig, ax = plt.subplots(figsize = (7,7))
sns.heatmap(cm_dummy_df.T, annot=True, annot_kws={"size": 15}, cmap="Oranges", vmin=0, vmax=800, fmt='.0f', linewidths=1, linecolor="white", cbar=False,
           xticklabels=["Iris-setosa","Iris-versicolor", 'Iris-virginica'], yticklabels=["Iris-setosa","Iris-versicolor", 'Iris-virginica'])
plt.ylabel("Predicted", fontsize=15)
plt.xlabel("Actual", fontsize=15)
ax.set_xticklabels(["Iris-setosa","Iris-versicolor", 'Iris-virginica'], fontsize=13)
ax.set_yticklabels(["Iris-setosa","Iris-versicolor", 'Iris-virginica'], fontsize=13)
plt.title("Confusion Matrix for 'Dummy' Classifier - Counts", fontsize=15)

plt.show()

#generate confusion matrix
cm_ann = confusion_matrix(y_test, y_predict)
#put it into a dataframe
cm_ann_df = pd.DataFrame(cm_ann)

#plot CM
fig, ax = plt.subplots(figsize = (7,7))
sns.heatmap(pd.DataFrame(cm_ann_df.T), annot=True, annot_kws={"size": 15}, cmap="Purples", vmin=0, vmax=500, fmt='.0f', linewidths=1, linecolor="white", cbar=False,
           xticklabels=["Iris-setosa","Iris-versicolor", 'Iris-virginica'], yticklabels=["Iris-setosa","Iris-versicolor", 'Iris-virginica'])
plt.ylabel("Predicted", fontsize=15)
plt.xlabel("Actual", fontsize=15)
ax.set_xticklabels(["Iris-setosa","Iris-versicolor", 'Iris-virginica'], fontsize=13)
ax.set_yticklabels(["Iris-setosa","Iris-versicolor", 'Iris-virginica'], fontsize=13)
plt.title("Confusion Matrix for ANN Classifier - Counts", fontsize=15)
plt.show()

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test,y_predict))
print(classification_report(y_test,y_predict))

import matplotlib.pyplot as plt
plt.scatter(X_test['sepal_length'], X_test['sepal_width'],c=y_test['type'])

import matplotlib.pyplot as plt
plt.scatter(X_test['sepal_length'], X_test['sepal_width'],c= y_predict['predict'])

print(__doc__)


# Code source: Gaël Varoquaux
#              Andreas Müller
# Modified for documentation by Jaques Grobler
# License: BSD 3 clause

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import make_moons, make_circles, make_classification
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis

h = .02  # step size in the mesh

names = ["Nearest Neighbors", "Linear SVM", "RBF SVM", "Gaussian Process",
         "Decision Tree", "Random Forest", "Neural Net", "AdaBoost",
         "Naive Bayes", "QDA"]

classifiers = [
    KNeighborsClassifier(3),
    SVC(kernel="linear", C=0.025),
    SVC(gamma=2, C=1),
    GaussianProcessClassifier(1.0 * RBF(1.0)),
    DecisionTreeClassifier(max_depth=5),
    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),
    MLPClassifier(alpha=1, max_iter=1000),
    AdaBoostClassifier(),
    GaussianNB(),
    QuadraticDiscriminantAnalysis()]

X, y = make_classification(n_features=2, n_redundant=0, n_informative=2,
                           random_state=1, n_clusters_per_class=1)
rng = np.random.RandomState(2)
X += 2 * rng.uniform(size=X.shape)
linearly_separable = (X, y)

datasets = [make_moons(noise=0.3, random_state=0),
            make_circles(noise=0.2, factor=0.5, random_state=1),
            linearly_separable
            ]

figure = plt.figure(figsize=(27, 9))
i = 1
# iterate over datasets
for ds_cnt, ds in enumerate(datasets):
    # preprocess dataset, split into training and test part
    X, y = ds
    X = StandardScaler().fit_transform(X)
    X_train, X_test, y_train, y_test = \
        train_test_split(X, y, test_size=.4, random_state=42)

    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5
    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                         np.arange(y_min, y_max, h))

    # just plot the dataset first
    cm = plt.cm.RdBu
    cm_bright = ListedColormap(['#FF0000', '#0000FF'])
    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
    if ds_cnt == 0:
        ax.set_title("Input data")
    # Plot the training points
    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
               edgecolors='k')
    # Plot the testing points
    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6,
               edgecolors='k')
    ax.set_xlim(xx.min(), xx.max())
    ax.set_ylim(yy.min(), yy.max())
    ax.set_xticks(())
    ax.set_yticks(())
    i += 1

    # iterate over classifiers
    for name, clf in zip(names, classifiers):
        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)
        clf.fit(X_train, y_train)
        score = clf.score(X_test, y_test)

        # Plot the decision boundary. For that, we will assign a color to each
        # point in the mesh [x_min, x_max]x[y_min, y_max].
        if hasattr(clf, "decision_function"):
            Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])
        else:
            Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]

        # Put the result into a color plot
        Z = Z.reshape(xx.shape)
        ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)

        # Plot the training points
        ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright,
                   edgecolors='k')
        # Plot the testing points
        ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright,
                   edgecolors='k', alpha=0.6)

        ax.set_xlim(xx.min(), xx.max())
        ax.set_ylim(yy.min(), yy.max())
        ax.set_xticks(())
        ax.set_yticks(())
        if ds_cnt == 0:
            ax.set_title(name)
        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'),
                size=15, horizontalalignment='right')
        i += 1

plt.tight_layout()
plt.show()



"""Recommendation Systems and using association rules to create a recommender system

a recommender system is a an algorithm whose aim is to provide most relevant information to a user by discovering patterns in the dataset.

* collaborative filterin
* content based filtering

![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/5/52/Collaborative_filtering.gif/300px-Collaborative_filtering.gif)

**User-based collaborative filtering:** In this model products are recommended to a user based on the fact that the products have been liked by users similar to the user. For example if Derrick and Dennis like the same movies and a new movie comes out that Derick likes,then we can recommend that movie to Dennis because Derrick and Dennis seem to like the same movies.

**Item-based collaborative filtering OR Content based filtering:** These systems identify similar items based on users’ previous ratings. For example if users A,B and C gave a 5 star rating to books X and Y then when a user D buys book Y they also get a recommendation to purchase book X because the system identifies book X and Y as similar based on the ratings of users A,B and C.

Association rules --> Apriori Algorithm

{Diapers} -> {Beer}

in the above example {Diapers} is called antecedent and {Beer} is consequent.

{Diaper, Gum} -> {Beer, Chips}


**Support** is the relative frequency that the rules show up.

**Confidence** which is a measure of reliability of the rule. P(Diaper|Beer)

**Lift** is the ratio of the observed support to that expected if the two rules were independent.
"""

import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings('ignore')

from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

url = 'https://raw.githubusercontent.com/ArashVafa/RecommendationSystemMovieLens/master/ratings.csv'

df = pd.read_csv(url)

url = 'https://raw.githubusercontent.com/ArashVafa/RecommendationSystemMovieLens/master/movies.csv'

movie_titles = pd.read_csv(url)

movie_titles.head()

df = pd.merge(df, movie_titles, on = 'movieId')

df.head()

ratings = pd.DataFrame(df.groupby('title')['rating'].mean())

ratings.head()

ratings['number_of_ratings'] = df.groupby('title')['rating'].count()
ratings.head()

import matplotlib.pyplot as plt
!matplot inline
ratings['rating'].hist(bins=50)

import seaborn as sns
sns.jointplot(x = 'rating', y = 'number_of_ratings', data = ratings)

basket = (df.groupby(['userId', 'title'])['rating']
          .sum().unstack().reset_index().fillna(0)
          .set_index('userId'))


basket.head()

def encode_units(x):
    if x <= 2:
        return 0
    if x >= 3:
        return 1
    else: 
        return 0

basket_sets = basket.applymap(encode_units)
basket_sets.head()

frequent_itemsets = apriori(basket_sets, min_support=0.1, use_colnames=True)

rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)
rules.head()

FCttt = rules[ rules['antecedents'] == ({'Fight Club (1999)'}) ]

FCttt.head()

FCttt.sort_values(by='support', ascending=False).head(10)

FCttt.sort_values(by='lift', ascending=False).head(10)

FCttt.sort_values(by='confidence', ascending=False).head(10)

rules[ (rules['lift'] >= 6) &
       (rules['confidence'] >= 0.8) ]

from IPython import display
from ipywidgets import interact, widgets

def recomm(Movie):
  RMovies = rules[ rules['antecedents'] == ({Movie}) ]
  RMovieList = RMovies.sort_values(by='confidence', ascending=False).head(10)
  RMovieListPrint = RMovieList['consequents']
   
  return(RMovieListPrint)


MoviesList = movie_titles['title']
interact(recomm, Movie=widgets.Dropdown(options=MoviesList))

FCPFttt = rules[ rules['antecedents'] == ({'Fight Club (1999)' ,'Pulp Fiction (1994)'}) ]

FCPFttt.head()

FCPFttt.sort_values(by='confidence', ascending=False).head(10)

def recomm(Movie1, Movie2):
  RMovies = rules[ rules['antecedents'] == ({Movie1, Movie2}) ]
  RMovieList = RMovies.sort_values(by='confidence', ascending=False).head(10)
  RMovieListPrint = RMovieList['consequents']
   
  return(RMovieListPrint)


MoviesList = movie_titles['title']
interact(recomm, Movie1=widgets.Dropdown(options=MoviesList), Movie2=widgets.Dropdown(options=MoviesList))

